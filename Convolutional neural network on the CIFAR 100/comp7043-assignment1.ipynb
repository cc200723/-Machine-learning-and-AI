{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0909b01-4e4d-4148-b4e6-6c321118d831",
   "metadata": {},
   "source": [
    "# COMP7043 Deep Learning and its Applications - Assignment 1\n",
    "-------\n",
    "\n",
    "In the programming part of assignment 1, you are going to train and experiment convolutional neural network on the CIFAR 100 dataset. The network is a Simple convolutional neural network defined with the `nn.Module` by yourself.\n",
    "\n",
    "When doing the homework, please read the description first then write your code in the pre-defined area. **Please import the needed package and ensure your code can run without modification under the Kaggle or Colab environment.**\n",
    "\n",
    "**Submit your notebook in a `*.ipynb` file.**\n",
    "\n",
    "## CIFAR-100\n",
    "The CIFAR-100 dataset is a commonly used benchmark dataset in the field of computer vision. It is an extension of the CIFAR-10 dataset and consists of 60,000 32x32 color images belonging to 100 different classes. Each class contains 600 images. The dataset is divided into 50,000 training images and 10,000 test images\n",
    "\n",
    "The classes in CIFAR-100 cover a wide range of objects and concepts, including animals, vehicles, household items, insects, plants, and more. Some examples of classes in CIFAR-100 are \"apple,\" \"bee,\" \"bus,\" \"dolphin,\" \"oak_tree,\" \"rose,\" \"sunflower,\" and \"whale.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92585bc9-eaad-497e-9be5-2ccd1941374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3f63e2-ca25-442f-ae2d-cc364681692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Transformation function that do following steps:\n",
    "# 1. RandomHorizontalFlip\n",
    "# 2. RandomRotation with any degree\n",
    "# 3. ToTensor\n",
    "# 4. Normalize with Mean (0.5071, 0.4867, 0.4408), Std (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Place your code here\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=(-180, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d817179-c2a0-4b0f-a704-dba125864622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-100 dataset with torchvision.datasets module and uses transform_train for transformation\n",
    "\n",
    "# Place your code below\n",
    "from torchvision import datasets\n",
    "\n",
    "dataset = datasets.CIFAR100(root=\"D:\\\\desktop\\\\torch tutorial\\\\data\", train=True, download=True, transform=transform_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cb01ad-78c4-4ee9-b854-a61e7bfa4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split function from the sklearn.model_selection to split the CIFAR dataset into train and test set with a ratio of 7:3\n",
    "# Then create two dataloader for the two sets and set the batch_size to 32, shuffle the training set but not the test set\n",
    "\n",
    "# Place your code below\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    " \n",
    "# Create data loaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cc2fd7-f189-46c0-b5e4-fbc5ef33ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyConvNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total parameters: 67546372\n"
     ]
    }
   ],
   "source": [
    "# Create a simple convolutional neural network with nn.Module,\n",
    "# the detailed structure is not restricted, but you need to include following layers in your network:\n",
    "# 1. nn.Linear\n",
    "# 2. nn.Conv2d\n",
    "# 3. Any Pooling Layer https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
    "# 4. Any Normalization Layer https://pytorch.org/docs/stable/nn.html#normalization-layers\n",
    "# 5. nn.Dropout\n",
    "# 6. Any Activation Layer https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Place your code below\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_classes=100):\n",
    "        super(MyConvNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3,  16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Normalization layer\n",
    "        self.norm = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 4096)\n",
    "        self.fc2 = nn.Linear(4096, out_classes)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.conv2(x))\n",
    "        X = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "        X = self.pool(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of the ConvNet\n",
    "model = MyConvNet(in_channels=3, out_classes=100)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n",
    "\n",
    "# Calculate the parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371d3c82-67a4-41df-8876-32ef5748d204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             448\n",
      "              ReLU-2           [-1, 16, 32, 32]               0\n",
      "         MaxPool2d-3           [-1, 16, 16, 16]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
      "              ReLU-5           [-1, 32, 16, 16]               0\n",
      "         MaxPool2d-6             [-1, 32, 8, 8]               0\n",
      "            Conv2d-7           [-1, 64, 16, 16]          18,496\n",
      "              ReLU-8           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-9             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-10           [-1, 64, 16, 16]             128\n",
      "           Linear-11                 [-1, 4096]      67,112,960\n",
      "             ReLU-12                 [-1, 4096]               0\n",
      "          Dropout-13                 [-1, 4096]               0\n",
      "           Linear-14                  [-1, 100]         409,700\n",
      "================================================================\n",
      "Total params: 67,546,372\n",
      "Trainable params: 67,546,372\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.92\n",
      "Params size (MB): 257.67\n",
      "Estimated Total Size (MB): 258.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network and print out the structure with summary module from torchsummary package\n",
    "\n",
    "# Place your code below\n",
    "from torchsummary import summary\n",
    "\n",
    "# Instantiate the network\n",
    "model = MyConvNet(in_channels=3, out_classes=100)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print the model summary\n",
    "summary(model,(3,32,32))  # Provide an example input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925cea85-de19-447d-8c8d-7f0f6fb3ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a loss function and optimizer for your model,\n",
    "# you can use whatever loss function and optimizer that is provided by torch.\n",
    "\n",
    "# Place your code below\n",
    "import torch.optim as optim\n",
    "\n",
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248ebcb9-b34e-4465-8800-d0035601a359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move all your model and loss function to the device that the model going to train.\n",
    "\n",
    "# Place your code below\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Move the loss function to the device\n",
    "criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de9618c7-b9ee-41b5-83f1-fb24e6fc363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.1452 - Acc: 0.9624\n",
      "Epoch 2 - Loss: 0.1508 - Acc: 0.9612\n",
      "Epoch 3 - Loss: 0.1491 - Acc: 0.9621\n",
      "Epoch 4 - Loss: 0.1383 - Acc: 0.9647\n",
      "Epoch 5 - Loss: 0.1167 - Acc: 0.9684\n",
      "Epoch 6 - Loss: 0.1249 - Acc: 0.9678\n",
      "Epoch 7 - Loss: 0.1229 - Acc: 0.9689\n",
      "Epoch 8 - Loss: 0.1254 - Acc: 0.9691\n",
      "Epoch 9 - Loss: 0.1389 - Acc: 0.9660\n",
      "Epoch 10 - Loss: 0.1192 - Acc: 0.9703\n"
     ]
    }
   ],
   "source": [
    "# Complete the following training iteration\n",
    "# then train your model on the training set for 5 iteration.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def metric(batch_predictions, batch_labels):\n",
    "    # Convert the predictions and labels to numpy arrays\n",
    "    _, predicted_labels = torch.max(batch_predictions, dim=1)\n",
    "    correct_predictions = (predicted_labels == batch_labels).sum().item()\n",
    "    # Calculate the prediction error\n",
    "    accuracy = correct_predictions / len(batch_labels)\n",
    "    return accuracy\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metric = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "################## Complete TODO below ##################\n",
    "    # Iterate over the batches in the dataloader\n",
    "    for batch in train_loader:\n",
    "        # TODO: sent batch data to device and clear gradients from previous iteration\n",
    "        \n",
    "        batch_data, batch_labels = batch[0], batch[1]\n",
    "        batch_data = batch_data.to(device)  # Sent batch data to the device (e.g., GPU)\n",
    "        batch_labels = batch_labels.to(device)  # Sent batch labels to the device\n",
    "        # print(batch_data.size())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        batch_predictions = model(batch_data)\n",
    "        # print(batch_predictions)\n",
    "        \n",
    "        # TODO: Calculate loss with the loss function\n",
    "        loss = criterion(batch_predictions, batch_labels)\n",
    "        epoch_loss += loss.item()\n",
    " \n",
    "        # TODO: Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        # Calculate and record metrics\n",
    "        batch_metric = metric(batch_predictions, batch_labels)\n",
    "        epoch_metric += batch_metric\n",
    " \n",
    "        batch_count += 1\n",
    "################## End of TODO part ##################\n",
    "    # Calculate average loss and metric for the epoch\n",
    "    avg_epoch_loss = epoch_loss / batch_count\n",
    "    avg_epoch_metric = epoch_metric / batch_count\n",
    " \n",
    "    # Print or log the metrics for analysis\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_epoch_loss:.4f} - Acc: {avg_epoch_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "405b7abb-ac6e-4072-ad27-d41045be5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuarcy: 0.1171\n"
     ]
    }
   ],
   "source": [
    "# Mimic the training iteration and complete the code for test phase (TODO),\n",
    "# the test phase should print out the accuracy score for your model on the test set.\n",
    "\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "epoch_metric = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "################## Complete TODO below ##################\n",
    "\n",
    "for batch in test_loader:\n",
    "    # TODO: sent batch data to device\n",
    "    inputs = batch[0].to(device)\n",
    "    labels = batch[1].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "\n",
    "    # TODO: Forward pass through the model\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    \n",
    "    # Calculate and record metrics\n",
    "    batch_metric = correct / labels.size(0) # TODO: Use the metric to calculate the accuracy\n",
    "    epoch_metric += batch_metric\n",
    "    batch_count += 1\n",
    "    \n",
    "################## End of TODO part ##################\n",
    "\n",
    "avg_epoch_metric = epoch_metric / batch_count\n",
    "print(f\"Test Accuarcy: {avg_epoch_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7432b-64b7-4b72-a7d5-0fc3e6f7ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
