{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e508368-ea5a-4afa-8053-4c3cbabdcbf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COMP7043 Deep Learning and its Applications - PyTorch Tutorial\n",
    "`VERSION 1.0` by Zhenghao WU ([ECWU](https://ecwuuuuu.com))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129631e-47de-48e5-8caa-f8f0f05cc893",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71652bd7-584e-479e-8796-72b26950a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118651c3-647a-4501-8551-0e49905e0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed PyTorch Version: 1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Installed PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb2d38-4d65-4b81-a3e3-e0fcfa45522d",
   "metadata": {},
   "source": [
    "## Tensors and Operations\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices (multi-dimensional array). In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensor can have different data types such as float, integer, or boolean, which is similar to NumPy’s ndarray and, except that **tensors can run on GPUs or other hardware accelerators**. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also **optimized for automatic differentiation**.\n",
    "\n",
    "### Creating / Initializing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51529ae-dae2-431b-91aa-128ce2ea0c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Direct create\n",
    "tensor_a = torch.zeros(2, 3)\n",
    "print(tensor_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65c593c-71f1-4901-9c03-6a226debee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# From Numpy Array\n",
    "import numpy as np\n",
    "\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_b = torch.from_numpy(numpy_array)\n",
    "print(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a965bf1b-d05d-449b-9227-055b8359501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# From another tensor\n",
    "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# When you use torch.tensor(), you create a new tensor\n",
    "# without any connection to the original tensor's computational\n",
    "# graph or gradient history. This means that the new tensor will\n",
    "# not have any gradients associated with it, and it will not\n",
    "# participate in any future gradient calculations or\n",
    "# backpropagation.\n",
    "\n",
    "# tensor_d = torch.tensor(tensor_c)\n",
    "# print(tensor_d)\n",
    "\n",
    "tensor_e = tensor_c.clone().detach()\n",
    "\n",
    "print(tensor_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931de67c-a6b9-4ec2-ac4f-bcca99ae2299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1831, 0.9155],\n",
      "        [0.2909, 0.0370],\n",
      "        [0.1526, 0.6224]])\n",
      "tensor([[-1.1225,  0.4461],\n",
      "        [ 0.1892,  0.5554]])\n",
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7],\n",
      "        [7, 7, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Fill in random or constant values\n",
    "tensor_f = torch.rand(3, 2)  # Uniform Distribution\n",
    "print(tensor_f)\n",
    "\n",
    "tensor_g = torch.randn(2, 2)  # Normal Distribution\n",
    "print(tensor_g)\n",
    "\n",
    "tensor_h = torch.full((3, 3), 7)  # Filled with constant value\n",
    "print(tensor_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa01881-20d8-4730-8782-daf8eca09a90",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "\n",
    "Tensors in PyTorch have several attributes that provide information about their shape, data type, and other properties. Here are some of the commonly used attributes of a PyTorch tensor.\n",
    "\n",
    "- Shape: It represents the size of each dimension of the tensor. You can access it using the shape attribute or the size() method. For example, tensor.shape or tensor.size() will return the shape of the tensor.\n",
    "- Datatype: It indicates the data type of the elements stored in the tensor. PyTorch supports various data types such as torch.float32, torch.int64, torch.bool, etc. You can access the data type of a tensor using the dtype attribute.\n",
    "- Device: It specifies the device (CPU or GPU) on which the tensor is stored. You can use the device attribute to check if the tensor is on the CPU or GPU.\n",
    "- `requires_grad`: This attribute indicates whether the tensor requires gradient computation for automatic differentiation. By default, tensors created directly from data do not require gradients. You can enable gradient tracking by setting `requires_grad=True` on a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d9d32d-271e-49c8-9322-aaea02cca813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape: torch.Size([2, 3, 4])\n",
      "tensor size: torch.Size([2, 3, 4])\n",
      "tensor dtype: torch.float32\n",
      "tensor dtype: torch.float64\n",
      "tensor device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros(2, 3, 4)\n",
    "print(f\"tensor shape: {tensor.shape}\")  # This is a attribute, not function\n",
    "print(f\"tensor size: {tensor.size()}\")\n",
    "\n",
    "print(f\"tensor dtype: {tensor.dtype}\")\n",
    "tensor = torch.zeros(2, 3, dtype=torch.float64)  # explicite define the dtype for tensor\n",
    "print(f\"tensor dtype: {tensor.dtype}\")\n",
    "\n",
    "tensor = torch.zeros(2, 3).to('cuda')\n",
    "print(f\"tensor device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d398b-5f23-454c-9705-2e18a0321c3a",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "In PyTorch, tensors support a wide range of operations (100+) for manipulating and performing computations on the data they contain. Including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html). Here are some example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f50c5ec-ed81-4916-8d05-bc719ce43875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor([[2, 3],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Standard numpy-like indexing and slicing: Tensors can be indexed and sliced using similar syntax as NumPy arrays.\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Indexing\n",
    "print(tensor[0, 1])\n",
    "\n",
    "# Slicing\n",
    "print(tensor[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ef3fdac-1f84-43c8-9d75-c024d371d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# Joining Tensors\n",
    "\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "concatenated = torch.cat((tensor1, tensor2), dim=0)\n",
    "print(concatenated)\n",
    "\n",
    "stacked = torch.stack((tensor1, tensor2), dim=0)\n",
    "print(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81dd4812-5449-4432-a3c2-a6b37bc384c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "reshaped = torch.reshape(tensor, (4,))\n",
    "print(reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7a21f78-4a6b-48ed-a860-ed8641b45cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic Operations\n",
    "\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "result = tensor1 + tensor2\n",
    "print(result)\n",
    "\n",
    "result = tensor1 * tensor2  # Element-wise\n",
    "print(result)\n",
    "\n",
    "result = torch.matmul(tensor1, tensor2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad462e-a263-43c2-9849-5e7ea1621436",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "### `nn.Module` Class\n",
    "\n",
    "It serves as a base class for all neural network modules in PyTorch and is used to define the architecture and behavior of the network. It provides a convenient way to organize the parameters of a model and define the forward pass computation. To create your own neural network using nn.Module, you need to define a subclass of nn.Module and override two key methods: `__init__` and `forward`. The `__init__` method is used to define the layers and modules of your network, while the `forward` method specifies the forward pass computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a72a438-3eff-4c93-988b-084807d108ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        # Define the layers and modules of your network\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass computation\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687a767-48db-4b23-8670-d0a9d9dce29d",
   "metadata": {},
   "source": [
    "In the `__init__` method, you can define any layers or modules you need for your network. In this example, we define two fully connected layers (`nn.Linear`) with specified input and output sizes.\n",
    "In the forward method, you specify the sequence of operations that will be applied to the input `x` during the forward pass. In this example, we apply the first linear layer (`self.fc1`), followed by a ReLU activation function (`torch.relu`), and finally the second linear layer (`self.fc2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28a80fe-0377-45c8-8db5-b0d2adb5cae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([[ 1.5166, -0.0746,  0.8971,  1.3344, -0.0321,  0.0018,  0.8580,  0.4342,\n",
      "         -0.2803, -1.6361]])\n",
      "output: tensor([[-0.3503,  0.0285]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "in_features = 10\n",
    "\n",
    "model = MyNetwork()\n",
    "\n",
    "input_data = torch.randn(batch_size, in_features)\n",
    "output = model(input_data)\n",
    "\n",
    "print(f\"input: {input_data}\\noutput: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1620d22-c5c7-42c3-8732-6063d5d958c8",
   "metadata": {},
   "source": [
    "### Predefined Layers\n",
    "PyTorch's nn module provides a wide range of predefined layers that you can use to build your neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243fbbc-a785-405d-8659-5a0af1299550",
   "metadata": {},
   "source": [
    "- `nn.Linear`: This layer implements a fully connected (linear) operation. It applies a linear transformation to the input data, where each input element is multiplied by a weight and summed with a bias term.\n",
    "- `nn.Conv2d`: This layer performs 2D convolutional operations on input data, commonly used in image processing tasks. It applies a set of learnable filters (kernels) to the input tensor to extract local features.\n",
    "- `nn.Dropout`: This layer implements dropout regularization, which randomly sets input elements to zero during training. Dropout helps prevent overfitting by reducing the interdependencies between neurons.\n",
    "- `nn.BatchNorm2d`: This layer performs batch normalization along the channels of a 2D input tensor. It normalizes the input by subtracting the mean and dividing by the standard deviation, which helps stabilize and accelerate the training process.\n",
    "- `nn.ReLU`: This activation function applies the Rectified Linear Unit (ReLU) element-wise to the input tensor. ReLU sets negative values to zero and keeps positive values unchanged.\n",
    "- `nn.Softmax`: This activation function applies the softmax operation to the input tensor, which normalizes the tensor into a probability distribution over the classes. It is commonly used for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174c94ef-f785-459a-b1e4-d9057c2be687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, out_classes):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "         \n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, out_classes)\n",
    "         \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        print(x.shape)\n",
    "         \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "         \n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec428285-0ba5-4a10-bedb-a0e9c3915ca6",
   "metadata": {},
   "source": [
    "### Model Summary and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3e4217-3d49-46eb-a6b9-8b5d89c0cbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Total parameters: 6424714\n"
     ]
    }
   ],
   "source": [
    "model = MyNetwork(in_channels=1, out_classes=10)\n",
    " \n",
    "# Print the network structure\n",
    "print(model)\n",
    " \n",
    "# Calculate the parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0cb17d-d831-46dc-a8e6-ddea93554619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a212aa3d-d8dd-4144-8bcf-f714cc42942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 28, 28])\n",
      "torch.Size([2, 50176])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "              ReLU-3           [-1, 64, 28, 28]               0\n",
      "           Dropout-4           [-1, 64, 28, 28]               0\n",
      "            Linear-5                  [-1, 128]       6,422,656\n",
      "              ReLU-6                  [-1, 128]               0\n",
      "           Dropout-7                  [-1, 128]               0\n",
      "            Linear-8                   [-1, 10]           1,290\n",
      "           Softmax-9                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 6,424,714\n",
      "Trainable params: 6,424,714\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.53\n",
      "Params size (MB): 24.51\n",
      "Estimated Total Size (MB): 26.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "model = MyNetwork(in_channels=1, out_classes=10)\n",
    "model = model.to(\"cuda:0\")\n",
    "# Print the model summary\n",
    "summary(model, (1, 28, 28))  # Provide an example input size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d49f8-529c-46dc-98b7-ccaff908be5b",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "PyTorch Dataset represents a dataset of input samples and their corresponding labels, while PyTorch DataLoader is responsible for efficiently loading the data from the dataset during training or inference.\n",
    "\n",
    "### Dataset Class\n",
    "\n",
    "The PyTorch torch.utils.data.Dataset class is a base class that you can inherit from to create your custom dataset.\n",
    "\n",
    "To create a dataset in PyTorch, you need to override the following methods of the Dataset class:\n",
    "\n",
    "- `__len__`: Returns the total number of data samples in the dataset.\n",
    "- `__getitem__`: Retrieves a specific data sample from the dataset, given its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91957c67-9bf9-45dc-8ca0-fea3fb5e1684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        # Implement any necessary preprocessing or data transformations here\n",
    "        # Return the preprocessed sample\n",
    "        return sample\n",
    " \n",
    "# Create an instance of your custom dataset\n",
    "data = [1, 2, 3, 4, 5]\n",
    "dataset = CustomDataset(data)\n",
    " \n",
    "# Access individual data samples\n",
    "for i in range(len(data)):\n",
    "    sample = dataset[i]\n",
    "    print(sample)  # Output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a34cc-0e72-493f-904b-c2a20ab2447e",
   "metadata": {},
   "source": [
    "### Built-in Datasets\n",
    "\n",
    "PyTorch provides several built-in datasets through the `torchvision.datasets` module. These datasets are commonly used for computer vision tasks. Here are some of the popular built-in datasets:\n",
    "\n",
    "- MNIST: Handwritten digit dataset.\n",
    "- CIFAR10 and CIFAR100: Small images dataset with 10 and 100 classes, respectively.\n",
    "- ImageNet: Large-scale image dataset with 1000 classes.\n",
    "- FashionMNIST: Fashion product images dataset.\n",
    "- COCO: Common Objects in Context dataset for object detection, segmentation, and captioning.\n",
    "- VOC: Visual Object Classes dataset for object detection, segmentation, and classification.\n",
    "- LSUN: Large-scale Scene Understanding dataset for scene classification and generation.\n",
    "- SVHN: Street View House Numbers dataset.\n",
    "- STL10: Small images dataset with 10 classes.\n",
    "- CelebA: Large-scale celebrity faces dataset.\n",
    "\n",
    "To use these built-in datasets in PyTorch, you need to follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ccf67e9-b299-4b85-afce-97c8de4752c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "dataset = datasets.MNIST(root=\"D://Desktop//data\", train=True, download=True)\n",
    " \n",
    "# Access the samples\n",
    "sample = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3db200e-dc4b-4175-b272-464fabb78b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(sample[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425782ab-c1ae-4615-8e71-15a15cba67cb",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Transformations in PyTorch are operations applied to data samples in a dataset. They are commonly used to preprocess or augment the data before feeding it into a machine learning model. PyTorch provides the torchvision.transforms module, which offers a variety of predefined transformations for computer vision tasks. Here are some commonly used transformations:\n",
    "\n",
    "- `ToTensor()`: Converts a PIL image or numpy array to a PyTorch tensor. It also scales the pixel values between 0 and 1.\n",
    "- `Normalize(mean, std)`: Normalizes a tensor by subtracting the mean and dividing by the standard deviation. The mean and std arguments specify the channel-wise means and standard deviations.\n",
    "- `Resize(size)`: Resizes the input PIL image to the specified size. It can take a single integer as an argument to resize the image's shorter side while maintaining its aspect ratio.\n",
    "- `CenterCrop(size)`: Crops the center portion of the image to the specified size.\n",
    "- `RandomCrop(size)`: Randomly crops the input image to the specified size.\n",
    "- `RandomHorizontalFlip()`: Randomly flips the input image horizontally with a probability of 0.5.\n",
    "- `RandomRotation(degrees)`: Rotates the input image by a random angle within the specified range.\n",
    "- `RandomResizedCrop(size)`: Randomly crops and resizes the input image to the specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a90a3c6e-b698-4be7-a3cb-afec3b11b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    " \n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(20),  # Crops the given image at the center.\n",
    "    transforms.Resize(28),  # Resize the input image to 28*28\n",
    "    # transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize tensor\n",
    "])\n",
    " \n",
    "# Create an instance of the dataset with transformations\n",
    "dataset = datasets.MNIST(root='D://Desktop//data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "819ecc22-1321-401d-8095-8d0bbba1d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d082d83130>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = [dataset[i] for i in range(4)]\n",
    "samples = [i[0] for i in samples]\n",
    "\n",
    "grid_img = torchvision.utils.make_grid(samples)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba622107-17e2-4c1f-b7d4-0e5c1ef7b872",
   "metadata": {},
   "source": [
    "### What is DataLoader and how to use\n",
    "\n",
    "In PyTorch, a DataLoader is an iterable that provides an interface to efficiently load data from a dataset during training or evaluation. It handles batch loading, shuffling, and other useful functionalities to facilitate the training process. The DataLoader takes a dataset as input and returns batches of data samples and their corresponding labels.\n",
    "\n",
    "The relationship between a dataset and a DataLoader is that the DataLoader wraps the dataset and provides an interface to access the data in batches. It abstracts away the details of data loading and allows you to focus on training your model.\n",
    "\n",
    "To use a DataLoader with the dataset you defined, you can follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecbe927a-4f55-4804-a9cf-103564335bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True, workers=16)\n",
    " \n",
    "for batch_data, batch_labels in dataloader:\n",
    "    # Use the batch_data and batch_labels for training or evaluation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a2f5f-1160-48cd-8c09-f770a72b3799",
   "metadata": {},
   "source": [
    "## Training and Optimization\n",
    "\n",
    "### Splitting data into training, validation, and test sets\n",
    "\n",
    "The common method we use for the dataset split is from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aea684e7-fd4c-4398-b22b-a378a3fdbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3ad98-cbf1-4418-b9d3-e2f547a33df4",
   "metadata": {},
   "source": [
    "In the example above, the dataset is split into 80% training data and 20% test data. Then, the training data is further split into 80% for training and 20% for validation.\n",
    "\n",
    "Once you have split your data into training, validation, and test sets, you can construct separate data loaders for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b593c93-d64a-4ad3-abaf-63d6fa15fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 1024\n",
    " \n",
    "# Create data loaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec2097-0cce-4c18-80f9-3a34533c0263",
   "metadata": {},
   "source": [
    "In the code above, train_data, val_data, and test_data represent the respective splits of your dataset. The `batch_size` parameter specifies the number of samples in each batch. You can adjust this value based on your computational resources and model requirements.\n",
    "\n",
    "The DataLoader class from PyTorch is used to create the data loaders. The shuffle=True argument is passed to the training loader to randomly shuffle the samples in each epoch, which helps in improving the model's generalization.\n",
    "\n",
    "### Defining loss functions (e.g., cross-entropy, mean squared error)\n",
    "\n",
    "After defining the data loader and model in a deep learning task, the next steps typically involve defining the loss function and optimizer. Here's a general outline of how to do that:\n",
    "\n",
    "The loss function measures the discrepancy between the predicted output of your model and the true labels. The choice of loss function depends on the specific task you are working on. Some common loss functions include:\n",
    "\n",
    "- Mean Squared Error (MSE): Suitable for regression tasks where the output is continuous.\n",
    "- Binary Cross-Entropy: Used for binary classification tasks where the output is a probability between 0 and 1.\n",
    "- Categorical Cross-Entropy: Appropriate for multi-class classification problems where the output is a probability distribution over multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21792141-6bb7-41b9-b006-0abd4f520090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f352fe-69aa-405e-b1ac-1943debaa80b",
   "metadata": {},
   "source": [
    "### Choosing and configuring optimizers (e.g., SGD, Adam)\n",
    "\n",
    "The optimizer is responsible for updating the model's parameters based on the computed gradients during the backpropagation process. It adjusts the parameters in the direction that minimizes the loss function. One commonly used optimizer is Stochastic Gradient Descent (SGD), but there are many other variants available, such as Adam, RMSprop, and Adagrad. PyTorch provides various optimizers in the torch.optim module. `To use an optimizer, you typically need to pass the model parameters and specify the learning rate`. Here's an example of defining an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe29d4d1-08a7-4c76-af20-b53e5c54ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    " \n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# Or\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8adc2-8a1b-46b0-91b3-cee5737d7e39",
   "metadata": {},
   "source": [
    "### Do Training\n",
    "\n",
    "Once you have defined the model, dataloader, loss function, and optimizer, you can proceed with training your model and calculating metrics for each batch and epoch. Here's an overview of the steps involved:\n",
    "\n",
    "Training Loop: \n",
    "- Iterate over your data for a specified number of epochs.\n",
    "- In each epoch, iterate over the batches of data provided by the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59f487ff-69d7-430c-b4d2-826010d03042",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2e36e-af40-4d24-9d26-c06b370a48a7",
   "metadata": {},
   "source": [
    "You'll typically follow these steps:\n",
    "\n",
    "- Iterate over the training dataset for a specific number of epochs.\n",
    "- Within each epoch, iterate over the batches of the dataset.\n",
    "- Perform the forward pass through the model to obtain predictions.\n",
    "- Calculate the loss using the defined loss function.\n",
    "- Perform the backward pass and update the model parameters using the optimizer.\n",
    "- Calculate and record the desired metrics for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72410842-76af-4893-ad1e-74f677095a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 1.8149 - Metric: 0.6442\n",
      "Epoch 2 - Loss: 1.5829 - Metric: 0.8778\n",
      "Epoch 3 - Loss: 1.5420 - Metric: 0.9195\n",
      "Epoch 4 - Loss: 1.5226 - Metric: 0.9392\n",
      "Epoch 5 - Loss: 1.5111 - Metric: 0.9505\n",
      "Epoch 6 - Loss: 1.5051 - Metric: 0.9566\n",
      "Epoch 7 - Loss: 1.4971 - Metric: 0.9652\n",
      "Epoch 8 - Loss: 1.4931 - Metric: 0.9690\n",
      "Epoch 9 - Loss: 1.4894 - Metric: 0.9725\n",
      "Epoch 10 - Loss: 1.4861 - Metric: 0.9762\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def metric(batch_predictions, batch_labels):\n",
    "    # Convert the predictions and labels to numpy arrays\n",
    "    _, predicted_labels = torch.max(batch_predictions, dim=1)\n",
    "    correct_predictions = (predicted_labels == batch_labels).sum().item()\n",
    "    # Calculate the prediction error\n",
    "    accuracy = correct_predictions / len(batch_labels)\n",
    "    return accuracy\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metric = 0.0\n",
    "    batch_count = 0\n",
    " \n",
    "    # Iterate over the batches in the dataloader\n",
    "    for batch in train_loader:\n",
    "        # Clear gradients from previous iteration\n",
    "        batch_data, batch_labels = batch[0], batch[1]\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        # print(batch_data.size())\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        # Forward pass\n",
    "        batch_predictions = model(batch_data)\n",
    "        # print(batch_predictions)\n",
    " \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(batch_predictions, batch_labels)\n",
    "        epoch_loss += loss.item()\n",
    " \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Calculate and record metrics\n",
    "        batch_metric = metric(batch_predictions, batch_labels)\n",
    "        epoch_metric += batch_metric\n",
    " \n",
    "        batch_count += 1\n",
    "     \n",
    "    # Calculate average loss and metric for the epoch\n",
    "    avg_epoch_loss = epoch_loss / batch_count\n",
    "    avg_epoch_metric = epoch_metric / batch_count\n",
    " \n",
    "    # Print or log the metrics for analysis\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_epoch_loss:.4f} - Metric: {avg_epoch_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312d589-9711-4e52-8544-f54b43fde500",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "To save a trained model in PyTorch, you can use the `torch.save()` function. This function allows you to save various components of the model, including the model's architecture, parameters, optimizer state, and any additional information you want to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba10c63c-c47c-4298-a0b3-962d9858cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03757ec2-1499-4915-8db8-946b89ee1097",
   "metadata": {},
   "source": [
    "In this example, the model object is saved in a file called \"`saved_model.pth`\". This file will contain the entire model, including its architecture, parameters, and other associated information.\n",
    "\n",
    "When you load the saved model, you can use the `torch.load()` function. Here's an example of how to load the saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a143b81f-f98c-4540-8bb5-9f23d5f33276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 28, 28]             640\n",
      "       BatchNorm2d-2           [-1, 64, 28, 28]             128\n",
      "              ReLU-3           [-1, 64, 28, 28]               0\n",
      "           Dropout-4           [-1, 64, 28, 28]               0\n",
      "            Linear-5                  [-1, 128]       6,422,656\n",
      "              ReLU-6                  [-1, 128]               0\n",
      "           Dropout-7                  [-1, 128]               0\n",
      "            Linear-8                   [-1, 10]           1,290\n",
      "           Softmax-9                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 6,424,714\n",
      "Trainable params: 6,424,714\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.53\n",
      "Params size (MB): 24.51\n",
      "Estimated Total Size (MB): 26.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.load('saved_model.pth')\n",
    "loaded_model = loaded_model.to(device)\n",
    "# Print the model summary\n",
    "summary(loaded_model, (1, 28, 28))  # Provide an example input size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887b327-e4ff-4b88-9f3f-c7c5e9813223",
   "metadata": {},
   "source": [
    "The `torch.load()` function returns the model object, which you can assign to a variable (loaded_model in this case). After loading, you can use the loaded model for inference, evaluation, or further training.\n",
    "\n",
    "Note that when you save the entire model using `torch.save()`, it saves the complete state of the model, including all parameters and buffers. However, it does not save the optimizer state by default. If you want to save and load the optimizer state as well, you can save it separately or include it in a dictionary along with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02b61ebf-aed5-46b2-b2b7-c7d8bfd150bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and optimizer together\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, 'saved_model-w-optimizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcea43-ed35-420d-878d-c03aa380eef2",
   "metadata": {},
   "source": [
    "To load the model and optimizer together, you can use `torch.load()` and then access the saved states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cebaca24-beed-47e2-b266-394f4a45ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('saved_model-w-optimizer.pth')\n",
    "opt_model = MyNetwork(in_channels=1, out_classes=10)\n",
    "opt_optimizer = optim.Adam(opt_model.parameters(), lr=0.001)\n",
    "opt_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b3154d-f5ea-426b-a97f-60a5f2b75b04",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "In PyTorch, you can use predefined models from the `torchvision.models` module, which provides a collection of popular pre-trained models for tasks such as image classification, object detection, and segmentation (There are other modules like Huggingface and FairSeq for NLP, LLM, and Multi-modal). These models are trained on large datasets like ImageNet and have learned useful features that can be leveraged for various computer vision tasks.\n",
    "\n",
    "To use a predefined model and download its pre-trained parameters, you can follow these steps:\n",
    "\n",
    "Import the necessary modules, and load a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "242ba6e5-c702-4a1e-8656-f29a813c1056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\32966/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:23<00:00, 4.38MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351881ee-88ef-4845-a7f5-af5c5f53c944",
   "metadata": {},
   "source": [
    "In this example, the ResNet-50 model is loaded with pre-trained weights. You can choose different models such as ResNet-18, VGG-16, etc. Here is the full model list:\n",
    "\n",
    "Table of all available classification weights: [Models and pre-trained weights — Torchvision 0.15 documentation (pytorch.org)](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights)\n",
    "\n",
    "After loaded such model with pre-trained parameters, you can use them just like the model you defined, but you have to know that the model is design for a specific tasks, which may not align with yours. So if you want to use the model on your task and fully utilize the pretrain effort, you can change the model and do transfer learning (fine-tuning). What you need to do is replacing or fine-tuning the last fully connected layer. For ResNet model the last layer typically corresponds to the classification layer for ImageNet's 1000 classes. If your task has a different number of classes, you need to adapt the last layer accordingly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa04c149-5c3c-49fc-a7c0-1c5cae0e05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6636c2-6eec-4792-9213-2c8529b12bf2",
   "metadata": {},
   "source": [
    "In this example, the last fully connected layer (`model.fc`) is replaced with a new linear layer that has num_classes output units.\n",
    "\n",
    "Depending on your task and the amount of available data, you may choose to freeze some layers to prevent their weights from being updated during training. This is particularly useful when you have limited data or when the pre-trained model is already well-suited to your task. For example, to freeze all layers except the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d8851fd-f911-41a2-93d0-504ed3668089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc.weight.requires_grad = True\n",
    "model.fc.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b57f5-b5e2-47e3-8c43-f9d555fc5800",
   "metadata": {},
   "source": [
    "In this example, all parameters (requires_grad) are set to False except for the weights and biases of the last fully connected layer (model.fc).\n",
    "\n",
    "With these steps, you can use a pre-trained model, download its pre-trained parameters, and modify it to suit your specific task. Once you've made the necessary modifications, you can train the model on your own dataset or use it for inference.\n",
    "\n",
    "\n",
    "> PS: The layer naming is not fully aligned for all model, like the Transformer model have a different name and structure for the output layer, you need to look in to the model first before the modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1df24dc-b693-4f22-8987-d9066ac2c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    " \n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(20),  # Crops the given image at the center.\n",
    "    transforms.Resize(28),  # Resize the input image to 28*28\n",
    "    # transforms.RandomHorizontalFlip(),  # Randomly flip horizontally\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize tensor\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) )\n",
    "])\n",
    " \n",
    "# Create an instance of the dataset with transformations\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    " \n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    " \n",
    "# Create data loaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30f650ce-c6fd-4703-87d6-127e201f6a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.3657 - Metric: 0.0987\n",
      "Epoch 2 - Loss: 2.3656 - Metric: 0.1010\n",
      "Epoch 3 - Loss: 2.3661 - Metric: 0.1014\n",
      "Epoch 4 - Loss: 2.3649 - Metric: 0.1003\n",
      "Epoch 5 - Loss: 2.3647 - Metric: 0.1018\n",
      "Epoch 6 - Loss: 2.3643 - Metric: 0.1024\n",
      "Epoch 7 - Loss: 2.3656 - Metric: 0.1000\n",
      "Epoch 8 - Loss: 2.3656 - Metric: 0.1008\n",
      "Epoch 9 - Loss: 2.3648 - Metric: 0.0996\n",
      "Epoch 10 - Loss: 2.3644 - Metric: 0.1021\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def metric(batch_predictions, batch_labels):\n",
    "    # Convert the predictions and labels to numpy arrays\n",
    "    _, predicted_labels = torch.max(batch_predictions, dim=1)\n",
    "    correct_predictions = (predicted_labels == batch_labels).sum().item()\n",
    "    # Calculate the prediction error\n",
    "    accuracy = correct_predictions / len(batch_labels)\n",
    "    return accuracy\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metric = 0.0\n",
    "    batch_count = 0\n",
    " \n",
    "    # Iterate over the batches in the dataloader\n",
    "    for batch in train_loader:\n",
    "        # Clear gradients from previous iteration\n",
    "        batch_data, batch_labels = batch[0], batch[1]\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        # print(batch_data.size())\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        # Forward pass\n",
    "        batch_predictions = model(batch_data)\n",
    "        # print(batch_predictions)\n",
    " \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(batch_predictions, batch_labels)\n",
    "        epoch_loss += loss.item()\n",
    " \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Calculate and record metrics\n",
    "        batch_metric = metric(batch_predictions, batch_labels)\n",
    "        epoch_metric += batch_metric\n",
    " \n",
    "        batch_count += 1\n",
    "     \n",
    "    # Calculate average loss and metric for the epoch\n",
    "    avg_epoch_loss = epoch_loss / batch_count\n",
    "    avg_epoch_metric = epoch_metric / batch_count\n",
    " \n",
    "    # Print or log the metrics for analysis\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_epoch_loss:.4f} - Metric: {avg_epoch_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030f380-e080-4bd9-a6f9-3130dfe41388",
   "metadata": {},
   "source": [
    "## GPU Acceleration\n",
    "\n",
    "GPU acceleration in PyTorch refers to leveraging the computational power of Graphics Processing Units (GPUs) to speed up training and inference processes. GPUs are highly parallel processors capable of performing multiple calculations simultaneously, making them well-suited for tasks involving large-scale matrix computations, such as deep learning.\n",
    "\n",
    "To utilize CUDA and GPU-enabled devices in PyTorch, you need to ensure that you have the appropriate hardware (an NVIDIA GPU) and the necessary software dependencies installed, including CUDA Toolkit and cuDNN. Once you have these prerequisites set up, you can follow these steps to move tensors and models to the GPU for faster computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05c4cd12-522d-48fc-ad69-b6491072a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78dc7e-9ec7-4ae4-a248-d8dca3355759",
   "metadata": {},
   "source": [
    "This code snippet checks if CUDA is available and assigns the device accordingly. If CUDA is available, the device will be set to \"cuda\"; otherwise, it will fall back to the CPU.\n",
    "\n",
    "- Move tensors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23c7eb68-f95b-4414-9439-21abfaa64bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f383cd-f066-4e63-8093-e04c5b351a5d",
   "metadata": {},
   "source": [
    "This line of code moves a PyTorch tensor to the GPU by calling the to() method and passing the device as an argument. After this operation, computations involving this tensor will be performed on the GPU.\n",
    "\n",
    "- Move models to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11958a3c-e942-438a-9eb1-ef23034cac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b61d1-ebb1-406c-a62a-f696649eb175",
   "metadata": {},
   "source": [
    "Similarly, you can move an entire PyTorch model to the GPU using the to() method. This ensures that all model parameters and computations are performed on the GPU.\n",
    "\n",
    "- Ensure inputs are also on the GPU:\n",
    "\n",
    "If you're passing inputs to the model during training or inference, make sure to move those tensors to the GPU as well. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f2f92f9-b5e9-47f5-967d-b39aa26750f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c65b20-4d32-44e1-961c-cf8be825605c",
   "metadata": {},
   "source": [
    "## Addition Topics\n",
    "\n",
    "### Random Seed\n",
    "\n",
    "The random seed is a crucial parameter when working with random number generation in PyTorch or any other deep learning framework. It is used to initialize the pseudorandom number generator (PRNG) algorithm, which is responsible for generating random numbers during model training.\n",
    "\n",
    "By setting a specific random seed, you can ensure reproducibility of your experiments. When the same random seed is used, the sequence of random numbers generated by the PRNG will be the same across multiple runs. This is important because deep learning models often involve random initialization of weights, dropout, data shuffling, and other stochastic operations. Reproducibility allows you to compare different model configurations, debug code, and share results with others.\n",
    "\n",
    "To preserve determinism when working with random seeds in PyTorch, there are a few key points to keep in mind:\n",
    "\n",
    "Setting the seed: Before initializing your model or performing any random operations, set the random seed using the `torch.manual_seed(seed)` function, where `seed` is an integer value. You can also set the random seed for numpy and other libraries if they are used in conjunction with PyTorch.\n",
    "GPU considerations: If you are using GPUs for training, be aware that additional steps may be necessary to ensure deterministic results. For example, you can use `torch.backends.cudnn.deterministic = True` and `torch.backends.cudnn.benchmark = False` to disable certain GPU optimizations that introduce non-determinism.\n",
    "NumPy interactions: If your code involves interactions between PyTorch and NumPy, be mindful that both libraries have their own random number generators. To maintain determinism, set the random seed for both libraries using `np.random.seed(seed)` and `torch.manual_seed(seed)`.\n",
    "Non-deterministic operations: While setting a random seed helps control the sources of randomness, some operations in PyTorch may still be non-deterministic, even with a fixed seed. Examples include certain GPU operations or multi-threaded code. In such cases, achieving full determinism may not be possible.\n",
    "Library versions: Ensure that you are using the same version of PyTorch and related libraries across different runs. Changes in library versions or underlying algorithms could affect the reproducibility of results, even with the same random seed.\n",
    "\n",
    "By being mindful of these considerations and setting the random seed appropriately, you can increase the reproducibility of your PyTorch model training experiments. Your can check more about reproducibility from Torch Document: [Reproducibility — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/notes/randomness.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "937bc923-c908-4948-ace9-ab9255038eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5f835-acc6-4453-ae85-96c7a3e653ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
